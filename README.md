# Teste EstagiÃ¡rio - Intuitive Care

## ğŸ“Œ VisÃ£o Geral
### **Projeto-Teste para vaga de estÃ¡gio**

Este projeto transforma dados brutos da ANS em informaÃ§Ãµes validadas, enriquecidas e agregadas.
Os dados sÃ£o armazenados em um banco de dados, acessÃ­veis via API e frontend interativo, alÃ©m de conter
documentaÃ§Ã£o completa e decisÃµes tÃ©cnicas justificadas

O projeto consiste em 4 etapas:
  -  01 - INTEGRAÃ‡ÃƒO COM API PÃšBLICA
  -  02 - TRANSFORMAÃ‡ÃƒO E VALIDAÃ‡ÃƒO DE DADOS
  -  03 - BANCO DE DADOS E ANÃLISE
  -  04 - API E INTERFACE WEB
  
Cada uma das etapas contÃ©m subtÃ³picos com instruÃ§Ãµes especÃ­ficas e direcionadas para o que fazer

## ğŸ› ï¸ Estrutura do Projeto

O projeto foi desenvolvido utilizando diferentes tecnologias a fim de facilitar o trajeto atÃ© o objetivo final

### Python

ResponsÃ¡vel por grande parte de processamento das 4 etapas do projeto incluindo:

- Leitura e escrita de arquivos CSV
- PadronizaÃ§Ã£o, validaÃ§Ã£o, filtragem, organizaÃ§Ã£o e anÃ¡lise de dados
- IntegraÃ§Ã£o de bases por meio de merge/join
- Tratamento de inconsistÃªncias
- GeraÃ§Ã£o de relatÃ³rios finais apÃ³s os tratamentos necessÃ¡rios

### Bibliotecas utilizadas:

- pandas â†’ manipulaÃ§Ã£o e anÃ¡lise de dados
- pathlib â†’ organizaÃ§Ã£o e portabilidade dos caminhos do projeto (padrÃ£o do python)
- re â†’ tratamento de strings e normalizaÃ§Ã£o de campos (padrÃ£o do python)

---
### MySQL

Utilizado somente na Etapa 03 para estruturar e organizar os dados importados dos arquivos CSV. Suas principais funÃ§Ãµes nesta etapa foram:

- Armazenamento estruturado das bases consolidadas
- ExecuÃ§Ã£o de consultas analÃ­ticas diretamente no banco
- ImportaÃ§Ã£o e manipulaÃ§Ã£o de dados via arquivos CSV
- CriaÃ§Ã£o de queries DDL para estruturar as tabelas
- Queries de importaÃ§Ã£o para inserir dados de arquivo CSV nas tabelas
- Queries analÃ­ticas para anÃ¡lise e geraÃ§Ã£o de relatÃ³rios

---
### Vue.js

Planejado para a camada de visualizaÃ§Ã£o do projeto e utilizando somente na Etapa 04

- CriaÃ§Ã£o de dashboards e tabelas interativas
- Consumo dos dados jÃ¡ processados pelo backend
- FacilitaÃ§Ã£o da anÃ¡lise dos resultados por usuÃ¡rios finais
- (NÃ£o utilizado diretamente na Etapa 02, mas definido como parte da arquitetura futura do sistema)

--- 


# ğŸ“‚ OrganizaÃ§Ã£o dos Arquivos

A estrutura do projeto foi organizada de forma modular para facilitar manutenÃ§Ã£o, leitura e evoluÃ§Ã£o:

### Intuitive_Care/Etapa01/
- ```src/``` -> scripts Python responsÃ¡veis pelo processamento
- ```data/``` -> subpastas com arquivos CSV relativos ao nome de cada planilha
- ```Planilhas_originais/``` -> arquivos baixado da API pÃºblica com dados relativos de 3 trimestres
- ```Planilhas_tratadas/``` -> arquivos pÃ³s primeira modificaÃ§Ã£o da Etapa 01 **("planilha_filtrada_sinistros.csv")** e para merge **("Relatorio_cadop.csv")**
- ```Resultado/``` -> arquivo pÃ³s-merge **("planilha_filtrada_sinistros.csv" e "Relatorio_cadop.csv")**
- ```ResultadoFinal/``` -> arquivo final organizado para ser usado na Etapa 02 **("consolidado_despesas.csv")**
- ```README.md``` -> documentar decisÃµes tÃ©cnicas e trade-offs da Etapa 01

### Intuitive_care02/Etapa02/ 
- ```consolidado_Etapa01/``` -> arquivo **"consolidado_despesas.csv"** da Etapa01 para realizar operaÃ§Ãµes
- ```dados_cadastrais/``` -> arquivo **"Relatorio_cadop.csv"** para merge/join
- ```pos_merge/``` â†’ arquivos gerados apÃ³s a integraÃ§Ã£o das bases 
- ```planilha_final/``` â†’ resultado dos arquivos agregados, ordenados, verificados e tratados
- ```src/``` -> scripts Python responsÃ¡veis pelo processamento
- ```README.md``` -> documentar decisÃµes tÃ©cnicas e trade-offs da Etapa02

### Intuitive_care03/Etapa03/
- ```data/``` -> arquivos CSV utilizados
- ```src``` -> arquivos SQL para criaÃ§Ã£o das diferentes queries 
- ```README.md``` -> documentar decisÃµes tÃ©cnicas e trade-offs da Etapa03

> âš ï¸ ObservaÃ§Ã£o: Testei diferentes arquiteturas de modularizaÃ§Ã£o e a estratÃ©gia usada na Etapa 03 foi a melhor. A organizaÃ§Ã£o com apenas data/ e src/ deixa o projeto mais objetivo e limpo, sem poluiÃ§Ã£o visual de mÃºltiplas pastas e arquivos desnecessÃ¡rios 
---


## ğŸ¯ DescriÃ§Ã£o das Etapas

  - **Etapa 01 â€“ IntegraÃ§Ã£o com API PÃºblica**

-> Processamento e ConsolidaÃ§Ã£o de dados

-> Buscar e baixar os arquivos de DemonstraÃ§Ãµes ContÃ¡beis dos Ãºltimos 3 trimestres disponÃ­veis na API PÃºblica: https://dadosabertos.ans.gov.br/FTP/PDA/ 

-> Juntar os dados dos 3 trimestres em um Ãºnico arquivo e filtrar apenas os dados que contenham ```Despesas com Eventos/Sinistros```

-> Criar e adicionar colunas especÃ­ficas e tratar inconsistÃªncias

-> Gerar o arquivo final ```consolidado_despesas.csv``` apÃ³s todo tratamento

---

  - **Etapa 02 - TransformaÃ§Ã£o e ValidaÃ§Ã£o de Dados**

-> ValidaÃ§Ã£o, enriquecimento e agregaÃ§Ã£o de dados

-> Validar CNPJ, RazÃ£o Social e valores numÃ©ricos positivos no ```consolidado_despesas.csv```

-> Baixar arquivo ```Relatorio_cadop.csv``` de Dados Cadastrais das Operadoras Ativas em: https://dadosabertos.ans.gov.br/FTP/PDA/operadoras_de_plano_de_saude_ativas/ 

-> Realizar merge entre ```consolidado_despesas.csv``` e ```Relatorio_cadop.csv``` usando CNPJ como chave

-> Criar e adicionar colunas adicionais, agrupar dados por colunas especÃ­ficas

-> Executar cÃ¡lculos, ordenar e organizar os dados

-> Gerar arquivo final ```despesas_agregadas.csv``` apÃ³s todas as operaÃ§Ãµes

---

  - **Etapa 03 - Banco de Dados e AnÃ¡lise**

-> Utilizar os arquivos ```consolidado_despesas.csv```, ```despesas_agregadas.csv``` e ```Relatorio_cadop.csv```

-> Criar queries DDL para estruturar tabelas necessÃ¡rias para cada arquivo CSV

-> Aplicar normalizaÃ§Ãµes e padronizaÃ§Ãµes nos dados

-> Criar queries de importaÃ§Ã£o para inserir dados nas tabelas, atentando-se ao encoding e tratamento de inconsistÃªncias

-> Desenvolver queries analÃ­ticas para responder perguntas de negÃ³cios







